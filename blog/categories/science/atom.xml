<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: science | @statwonk]]></title>
  <link href="http://statwonk.github.io/blog/categories/science/atom.xml" rel="self"/>
  <link href="http://statwonk.github.io/"/>
  <updated>2014-02-16T13:42:48-05:00</updated>
  <id>http://statwonk.github.io/</id>
  <author>
    <name><![CDATA[Christopher P. Peters]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[P-value does not stand for profit.]]></title>
    <link href="http://statwonk.github.io/blog/2013/10/06/the-p-in-p-value-does-not-stand-for-profit-dot/"/>
    <updated>2013-10-06T16:26:00-04:00</updated>
    <id>http://statwonk.github.io/blog/2013/10/06/the-p-in-p-value-does-not-stand-for-profit-dot</id>
    <content type="html"><![CDATA[<p>With the data science revolution, a whole new round of statisticans will be thrust into dealing with business problems.  What they&rsquo;re bound to find is that their beloved p-value is not going to stand up to the market as it does in academic journals.  For the past thirty years, the computational cost of the simple t-test has fallen dramatically, while the academic reward of publication has stayed the same.  The result is that more newly minted analysts than ever are confused by the blank stare they receive from their manager when they tell the manager, &ldquo;these two samples are statistically significantly different at the 5% level.&rdquo;  The manager asks, &ldquo;{% raw %}<span style="background-color:#ffe026">so what? How much profit will we make?</span>{% endraw %}&rdquo; To which the statistician replies, &ldquo;{% raw %}<span style="background-color:#ffe026">the p-value is less than 5%.</span>{% endraw %}&rdquo;</p>

<p>{% raw %}<span style="background-color:#ffe026"></span>{% endraw %}
Here&rsquo;s one simple example. Suppose I have two site variations I want to test.  I&rsquo;d like to test a new red button against our old green button.  I&rsquo;m interested to know if the red button outperforms green with respect to clicks.</p>

<p>We&rsquo;ll simulate this with the <code>R</code> language.  We can draw a <code>0</code> or <code>1</code> with known probabilities to model our buttons.  I&rsquo;ll set the conversion rate for the green button at 3% and 5% for our new red button, a 66% percent improvement.</p>

<p>{% codeblock lang:r %}</p>

<p>old_green_button &lt;&ndash; rbinom(n = 100000, 1, p = 0.03)
new_red_button &lt;&ndash; rbinom(n = 100000, 1, p = 0.05)</p>

<p>prop.test(table(old_green_button, new_red_button))
{% endcodeblock %}</p>

<p>In the code above, I first randomly generate <code>100k</code> binomial outcomes (really bernoulli r.v.) for each button. Remember, the conversion rates are 3% and 5%, respectively.  Next, I apply the Chi-square test for equality of proportions.  This is the standard statistical test for testing if two proportions (conv. rates) come from the same population.  That is, should I expect these two buttons to yield a different conversion rate (and profit) going forward?</p>

<p>Below are the results from one run, and despite the new red button being 66%
better than the green button, our p-value sits at 90%, well above the
traditional statistician&rsquo;s 5% p-value cutoff.</p>

<p>{% codeblock lang:r %}</p>

<p>2-sample test for equality of proportions with continuity correction</p>

<p>data:  table(old_green_button, new_red_button)
X-squared = 0.0185, df = 1, p-value = 0.8919
alternative hypothesis: two.sided
95 percent confidence interval:
 -0.008662492  0.007240556
 sample estimates:</p>

<pre><code>prop 1    prop 2 
0.9498581 0.9505691 
</code></pre>

<p>{% endcodeblock %}</p>

<p>Now, I&rsquo;ll repeat this experiment 1000 times and show you how the p-value
shakes out.</p>

<p>{% codeblock lang:r %}
library(ggplot2)
library(ggthemes)</p>

<p>times &lt;&ndash; 1000
p_values &lt;&ndash; rep(NA, times)
for(i in 1:times){
  set.seed(i)
  old_green_button &lt;&ndash; rbinom(n = 100000, 1, p = 0.03)
  new_red_button &lt;&ndash; rbinom(100000, 1, p = 0.05)
  p_values[i] &lt;&ndash; prop.test(table(old_green_button,</p>

<pre><code>                             new_red_button))$p.value
</code></pre>

<p>  print(i)
}</p>

<p>df &lt;&ndash; as.data.frame(p_values)
ggplot(df, aes(x = p_values)) +
  geom_density(fill = &ldquo;grey50&rdquo;, alpha = 0.5) +
  geom_vline(xintercept = 0.05, colour = &ldquo;red&rdquo;, size = 2) +
  ggtitle(&ldquo;Only 6% of p-values are less than 0.05&rdquo;) +</p>

<pre><code>ylab("Density") +
xlab("P-values") +
</code></pre>

<p>  theme_few() +
  theme(axis.text = element_text(size = 15),</p>

<pre><code>    plot.title = element_text(size = 20))
</code></pre>

<p>{% endcodeblock %}</p>

<p>The result shows that only 6% of p-values will be less than 0.05%!  The
sacred statistical decision rule would have you leave a 66% percent
conversion rate increase on the table and this is why p-value does not stand for profit.</p>

<p><img src="http://i.imgur.com/9vl6z1x.png"></img></p>

<p>In my next post, I&rsquo;ll analyze exactly why using p-values above other
methods leads the data scientist astray.  I&rsquo;ll introduce how I use Bayesian
analysis to avoid the p-value pitfall.</p>
]]></content>
  </entry>
  
</feed>
