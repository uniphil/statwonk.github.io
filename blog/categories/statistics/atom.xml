<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: statistics, | @statwonk]]></title>
  <link href="http://statwonk.github.io/blog/categories/statistics/atom.xml" rel="self"/>
  <link href="http://statwonk.github.io/"/>
  <updated>2014-02-16T13:42:48-05:00</updated>
  <id>http://statwonk.github.io/</id>
  <author>
    <name><![CDATA[Christopher P. Peters]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Taking a quick peek at FDA adverse drug reactions]]></title>
    <link href="http://statwonk.github.io/blog/2013/10/26/taking-a-quick-peek-at-fda-drug-reactions/"/>
    <updated>2013-10-26T00:07:00-04:00</updated>
    <id>http://statwonk.github.io/blog/2013/10/26/taking-a-quick-peek-at-fda-drug-reactions</id>
    <content type="html"><![CDATA[<p>About a week ago my doctor put me on a new medication.  The drug is one that is known to have some unpleasant effects as you get used to it.  I was curious to see what the reported adverse reactions (to the U.S. FDA) were, so I decided to take a look at the data.</p>

<p>The data actually wasn&rsquo;t very easy to find, but here&rsquo;s the current location: <a href="http://www.fda.gov/Drugs/GuidanceComplianceRegulatoryInformation/Surveillance/AdverseDrugEffects/ucm082193.htm">http://www.fda.gov/Drugs/GuidanceComplianceRegulatoryInformation/Surveillance/AdverseDrugEffects/ucm082193.htm</a>.</p>

<p>Wow, look at the SEO value of that URL!</p>

<p>It&rsquo;s pretty unbelievable to me that the latest data is now more than 10 months old. We&rsquo;ll have to see what the FDA says,</p>

<blockquote class="twitter-tweet tw-center-align"><p><a
href="https://twitter.com/FDA_Drug_Info">@FDA_Drug_Info</a> This info seems
stale, what&#39;s going on? <a
href="http://t.co/fVAkiJzS9i">http://t.co/fVAkiJzS9i</a></p>&mdash; Christopher
Peters (@statwonk) <a
href="https://twitter.com/statwonk/statuses/393945145030094848">October 26,
2013</a></blockquote>


<script async src="http://statwonk.github.io//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>So I took a look at the drug reaction info and the top twenty reactions seemed relatively benign, so I feel a bit better.  It&rsquo;s a pretty simple way to go about things, but I&rsquo;d encourage you to take a look at some <a href="http://www.accessdata.fda.gov/scripts/cder/drugsatfda/index.cfm">FDA studies of drugs you take</a>.  The few I&rsquo;ve looked at are appallingly deficient.  Where are the power studies, yo?  If a study has a 5% likelihood of detecting a 20% increase in heart attacks, you&rsquo;d never know &mdash; that&rsquo;s why you should NOT blindly trust p-values > 0.05.  It doesn&rsquo;t mean that there isn&rsquo;t an effect and the study may not have had a good chance of detecting the increased risk from the get-go.</p>

<p>You can find my little script on <a href="https://github.com/statwonk/FDA-adverse-drug-reactions">Github here</a>.</p>

<p>I would love to collaborate with someone on this, let&rsquo;s dig into this data!</p>

<p>Perhaps data mine to find not well known drug interactions?</p>

<p>{% codeblock lang:r %}</p>

<h1>Author: Christopher Peters</h1>

<h1>Twitter: @statwonk</h1>

<h1>Analysis of adverse reported drug events from FDA database</h1>

<h1>Data found here: <a href="http://www.fda.gov/Drugs/GuidanceComplianceRegulatoryInformation/Surveillance/AdverseDrugEffects/ucm082193.htm">http://www.fda.gov/Drugs/GuidanceComplianceRegulatoryInformation/Surveillance/AdverseDrugEffects/ucm082193.htm</a></h1>

<p>patient &lt;&ndash; read.table(&ldquo;~/Downloads/faers_ascii_2012q4/ascii/demo12q4.txt&rdquo;, sep = &ldquo;$&rdquo;, header = T, fill = T, quote = &ldquo;&rdquo;)
drug &lt;&ndash; read.table(&ldquo;~/Downloads/faers_ascii_2012q4/ascii/drug12q4.txt&rdquo;, sep = &ldquo;$&rdquo;, header = T, fill = T, quote = &ldquo;&rdquo;)
reaction &lt;&ndash; read.table(&ldquo;~/Downloads/faers_ascii_2012q4/ascii/reac12q4.txt&rdquo;, sep = &ldquo;$&rdquo;, header = T, fill = T, quote = &ldquo;&rdquo;)
outcomes &lt;&ndash; read.table(&ldquo;~/Downloads/faers_ascii_2012q4/ascii/outc12q4.txt&rdquo;, sep = &ldquo;$&rdquo;, header = T, fill = T, quote = &ldquo;&rdquo;)</p>

<h1>You can find individual drugs and their reported adverse by specifying there names below</h1>

<h1>There are commonly many names for a drug, see below where I put three grepl statements</h1>

<h1>with text like, &ldquo;put drug brand name here&rdquo;, you can replace this with drug brand names</h1>

<h1>you can add or remote grepl statements depending on the number of brand names you want to look over.</h1>

<p>df &lt;&ndash; drug[(grepl(&ldquo;put drug brand name 1 here&rdquo;, drug$drugname, ignore.case = T) | # drug is likely to be entered as many different brand names, use this to capture them individually</p>

<pre><code>          grepl("put drug brand name 2 here", drug$drugname, ignore.case = T) | # enter drug names here, add or remove grepl() as needed with "or" statements \
          grepl("put drug brand name 3 here ... add more grepl statements as necessary", drug$drugname, ignore.case = T)) &amp; drug$drug_seq == 1, ] # drug seq 1 == suspect drug of many possible that patient is taking
</code></pre>

<p>df &lt;&ndash; merge(df, reaction, by = &ldquo;primaryid&rdquo;) # let&rsquo;s merge the drug file with reactions
df &lt;&ndash; merge(df, outcomes, by = &ldquo;primaryid&rdquo;) # we&rsquo;ll bring in outcomes, too
df2 &lt;&ndash; as.data.frame(table(df$pt, df$outc_code)) # count the instances of reactions and their outcomes
names(df2) &lt;&ndash; c(&ldquo;reaction&rdquo;, &ldquo;outcome&rdquo;, &ldquo;count&rdquo;)
df2 &lt;&ndash; df2[order(df2$count, decreasing = T), ]
head(df2, 20) # top 20 reactions
{% endcodeblock %}</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[P-value does not stand for profit.]]></title>
    <link href="http://statwonk.github.io/blog/2013/10/06/the-p-in-p-value-does-not-stand-for-profit-dot/"/>
    <updated>2013-10-06T16:26:00-04:00</updated>
    <id>http://statwonk.github.io/blog/2013/10/06/the-p-in-p-value-does-not-stand-for-profit-dot</id>
    <content type="html"><![CDATA[<p>With the data science revolution, a whole new round of statisticans will be thrust into dealing with business problems.  What they&rsquo;re bound to find is that their beloved p-value is not going to stand up to the market as it does in academic journals.  For the past thirty years, the computational cost of the simple t-test has fallen dramatically, while the academic reward of publication has stayed the same.  The result is that more newly minted analysts than ever are confused by the blank stare they receive from their manager when they tell the manager, &ldquo;these two samples are statistically significantly different at the 5% level.&rdquo;  The manager asks, &ldquo;{% raw %}<span style="background-color:#ffe026">so what? How much profit will we make?</span>{% endraw %}&rdquo; To which the statistician replies, &ldquo;{% raw %}<span style="background-color:#ffe026">the p-value is less than 5%.</span>{% endraw %}&rdquo;</p>

<p>{% raw %}<span style="background-color:#ffe026"></span>{% endraw %}
Here&rsquo;s one simple example. Suppose I have two site variations I want to test.  I&rsquo;d like to test a new red button against our old green button.  I&rsquo;m interested to know if the red button outperforms green with respect to clicks.</p>

<p>We&rsquo;ll simulate this with the <code>R</code> language.  We can draw a <code>0</code> or <code>1</code> with known probabilities to model our buttons.  I&rsquo;ll set the conversion rate for the green button at 3% and 5% for our new red button, a 66% percent improvement.</p>

<p>{% codeblock lang:r %}</p>

<p>old_green_button &lt;&ndash; rbinom(n = 100000, 1, p = 0.03)
new_red_button &lt;&ndash; rbinom(n = 100000, 1, p = 0.05)</p>

<p>prop.test(table(old_green_button, new_red_button))
{% endcodeblock %}</p>

<p>In the code above, I first randomly generate <code>100k</code> binomial outcomes (really bernoulli r.v.) for each button. Remember, the conversion rates are 3% and 5%, respectively.  Next, I apply the Chi-square test for equality of proportions.  This is the standard statistical test for testing if two proportions (conv. rates) come from the same population.  That is, should I expect these two buttons to yield a different conversion rate (and profit) going forward?</p>

<p>Below are the results from one run, and despite the new red button being 66%
better than the green button, our p-value sits at 90%, well above the
traditional statistician&rsquo;s 5% p-value cutoff.</p>

<p>{% codeblock lang:r %}</p>

<p>2-sample test for equality of proportions with continuity correction</p>

<p>data:  table(old_green_button, new_red_button)
X-squared = 0.0185, df = 1, p-value = 0.8919
alternative hypothesis: two.sided
95 percent confidence interval:
 -0.008662492  0.007240556
 sample estimates:</p>

<pre><code>prop 1    prop 2 
0.9498581 0.9505691 
</code></pre>

<p>{% endcodeblock %}</p>

<p>Now, I&rsquo;ll repeat this experiment 1000 times and show you how the p-value
shakes out.</p>

<p>{% codeblock lang:r %}
library(ggplot2)
library(ggthemes)</p>

<p>times &lt;&ndash; 1000
p_values &lt;&ndash; rep(NA, times)
for(i in 1:times){
  set.seed(i)
  old_green_button &lt;&ndash; rbinom(n = 100000, 1, p = 0.03)
  new_red_button &lt;&ndash; rbinom(100000, 1, p = 0.05)
  p_values[i] &lt;&ndash; prop.test(table(old_green_button,</p>

<pre><code>                             new_red_button))$p.value
</code></pre>

<p>  print(i)
}</p>

<p>df &lt;&ndash; as.data.frame(p_values)
ggplot(df, aes(x = p_values)) +
  geom_density(fill = &ldquo;grey50&rdquo;, alpha = 0.5) +
  geom_vline(xintercept = 0.05, colour = &ldquo;red&rdquo;, size = 2) +
  ggtitle(&ldquo;Only 6% of p-values are less than 0.05&rdquo;) +</p>

<pre><code>ylab("Density") +
xlab("P-values") +
</code></pre>

<p>  theme_few() +
  theme(axis.text = element_text(size = 15),</p>

<pre><code>    plot.title = element_text(size = 20))
</code></pre>

<p>{% endcodeblock %}</p>

<p>The result shows that only 6% of p-values will be less than 0.05%!  The
sacred statistical decision rule would have you leave a 66% percent
conversion rate increase on the table and this is why p-value does not stand for profit.</p>

<p><img src="http://i.imgur.com/9vl6z1x.png"></img></p>

<p>In my next post, I&rsquo;ll analyze exactly why using p-values above other
methods leads the data scientist astray.  I&rsquo;ll introduce how I use Bayesian
analysis to avoid the p-value pitfall.</p>
]]></content>
  </entry>
  
</feed>
